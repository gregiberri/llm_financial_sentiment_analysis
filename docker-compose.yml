version: '3.0'
services: 
  llm_sentiment: 
    image: llm_sentiment
    build:
      context: .
    container_name: llm_sentiment
    ports: 
      - "5577:8888"
      - "5578:22"
    runtime: nvidia
    volumes: 
      - ./:/code
    depends_on:
      - oobabooga
  oobabooga:
    image: oobabooga
    container_name: oobabooga
    build:
      context: oobabooga
      dockerfile: Dockerfile
    command: ["TechxGenus_Meta-Llama-3-70B-Instruct-GPTQ", "AutoGPTQ"]
    runtime: nvidia
    volumes:
      - ./models:/models
    ports:
      - 7860:7860
      - 5000:5000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
